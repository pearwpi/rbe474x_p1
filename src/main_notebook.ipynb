{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593eb79b",
   "metadata": {},
   "source": [
    "# P1: Nifty Neural Networks!\n",
    "\n",
    "\n",
    "## Table Of Content\n",
    "\n",
    "1. Introduction\n",
    "2. Preliminaries\n",
    "3. Software Setup\n",
    "4. Implementation\n",
    "5. Grading Rubric\n",
    "6. Report guidelines\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Neural networks, at their core, function like any other mathematical function that can be evaluated. The process of evaluating a neural network is referred to as the forward pass. During this step, inputs are passed through the network layers, and outputs are generated.\n",
    "\n",
    "To optimize the network's performance, its weights and biases need to be adjusted. This is done through a process called backward propagation (or backpropagation). In this step, the gradients of the loss function with respect to each parameter are calculated, and these gradients are subtracted from the corresponding weights and biases, allowing the network to learn and improve its predictions.\n",
    "\n",
    "In this assignment, you will dive into the implementation of custom layers in PyTorch. Specifically, you will focus on coding the forward pass and computing the gradients necessary for the backward pass. Before you begin, make sure to review the grading rubric to understand the criteria for evaluation.\n",
    "\n",
    "## 2. Preliminaries\n",
    "\n",
    "### CIFAR10 Dataset\n",
    "\n",
    "CIFAR-10 is a dataset consisting of 60000, 32Ã—32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. More details about the datset can be found [here](http://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Sample images from each class of the CIFAR-10 dataset is shown below:\n",
    "\n",
    "![CIFAR 10](./artifacts/cifar10.png)\n",
    "\n",
    "In this project, you will classify images into these 10 classes using the provided pipeline,loaders and helper classes.\n",
    "\n",
    "Additionally, you are expected to generate a confusion matrix to evaluate your model's performance. For guidance on plotting a confusion matrix in PyTorch, please refer to this [resource](https://stackoverflow.com/questions/74020233/how-to-plot-confusion-matrix-in-pytorch).\n",
    "\n",
    "### Linear Layer\n",
    "A linear layer in a neural network performs a linear transformation of the input data. It is defined by the following components:\n",
    "\n",
    "1. Weights\n",
    "2. Biases\n",
    "\n",
    "More details below,\n",
    "\n",
    "[Pytorch Linear Layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n",
    "\n",
    "You can find information about the dimension of weights and biases in custom_layers.py\n",
    "\n",
    "### Soft Max\n",
    "The Softmax function is commonly used in neural networks for multi-class classification problems. It converts a vector of raw scores (logits) into probabilities, making it possible to interpret the output as the likelihood of each class.\n",
    "\n",
    "[Sample implementation](https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python)\n",
    "\n",
    "More details [here](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html).\n",
    "\n",
    "### Convolutional Layer\n",
    "\n",
    "A convolutional layer is a fundamental building block in Convolutional Neural Networks (CNNs) used primarily for processing grid-like data such as images. It applies convolution operations to detect local features in the input.\n",
    "\n",
    "Although it is called a convolutional layer, the PyTorch implementation of conv2d does not actually perform a convolution in the mathematical sense. Instead, it performs a cross-correlation operation, where the kernel is not flipped. This distinction is important to note, but for most deep learning projects including this one, cross-correlation is perfectly fine as the weights will automatically adjust during training.\n",
    "\n",
    "For more details, refer to [P0](https://rbe549.github.io/rbe474x/fall2024a/proj/p0/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f305e",
   "metadata": {},
   "source": [
    "## 3. Software Setup\n",
    "\n",
    "Use a code editor like VSCode and open this entire folder.\n",
    "\n",
    "For each part, you will be implementing the corresponding layers in custom_layers.py\n",
    "\n",
    "The code will automatically be tested with test.py. \n",
    "\n",
    "To run the test, open a terminal in the current folder and run,\n",
    "\n",
    "`pytest -s -v test.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c586079",
   "metadata": {},
   "source": [
    "## 4. Implementation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d925fff4",
   "metadata": {},
   "source": [
    "### Part1 : Implement Your Custom Layers for Multi Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb96b9",
   "metadata": {},
   "source": [
    "Open custom_layers.py and implement a fully connected, relu and softmax layer.\n",
    "\n",
    "Verify it by running the below code. Feel free to modify the below snippet. But do not modify my test.py\n",
    "\n",
    "For more information about supplying gradients, please refer to [examples_autograd](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc90238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import networks as net\n",
    "importlib.reload(net)\n",
    "\n",
    "print(\"\\nLinear\")\n",
    "u = torch.rand((1, 10))\n",
    "customLayer = net.CustomLinear(10, 2)\n",
    "inbuiltLayer = nn.Linear(in_features=10, out_features=2)\n",
    "\n",
    "inbuiltLayer.weight.data.copy_(customLayer.weight.data)\n",
    "inbuiltLayer.bias.data.copy_(customLayer.bias.data)\n",
    "\n",
    "y_custom = customLayer(u)\n",
    "y_inbuilt = inbuiltLayer(u)\n",
    "print(\"Inference for linear layer\")\n",
    "print(y_custom)\n",
    "print(y_inbuilt)\n",
    "\n",
    "lossFunc = nn.MSELoss()\n",
    "\n",
    "loss_custom = lossFunc(y_custom, torch.zeros_like(y_custom))\n",
    "loss_in = lossFunc(y_inbuilt, torch.zeros_like(y_inbuilt))\n",
    "\n",
    "loss_custom.backward()\n",
    "loss_in.backward()\n",
    "\n",
    "print(\"\\ngraidents for linear layer\")\n",
    "print(customLayer.weight.grad)\n",
    "print(inbuiltLayer.weight.grad)\n",
    "\n",
    "print(customLayer.bias.grad)\n",
    "print(inbuiltLayer.bias.grad)\n",
    "\n",
    "# RELU\n",
    "print(\"\\nRELU\")\n",
    "u1 = torch.rand((1, 10), requires_grad=True)\n",
    "u2 = u1.detach().clone()\n",
    "u2.requires_grad_()\n",
    "\n",
    "customLayer = net.CustomReLU()\n",
    "inbuiltLayer = nn.ReLU()\n",
    "\n",
    "y_custom = customLayer(u1)\n",
    "y_inbuilt = inbuiltLayer(u2)\n",
    "\n",
    "loss_custom = lossFunc(y_custom, torch.zeros_like(y_custom))\n",
    "loss_in = lossFunc(y_inbuilt, torch.zeros_like(y_inbuilt))\n",
    "\n",
    "loss_custom.backward()\n",
    "loss_in.backward()\n",
    "\n",
    "print(\"inference\")\n",
    "print(y_custom, y_inbuilt)\n",
    "\n",
    "print(\"gradients of loss relative to the input\")\n",
    "print(u1.grad)\n",
    "print(u2.grad)\n",
    "\n",
    "# SOFTMAX\n",
    "print(\"\\n SoftMax\")\n",
    "\n",
    "u1 = torch.rand((1, 3), requires_grad=True)\n",
    "u2 = u1.detach().clone()\n",
    "u2.requires_grad_()\n",
    "customLayer = net.CustomSoftmax(1)\n",
    "inbuiltLayer = nn.Softmax()\n",
    "\n",
    "y_custom = customLayer(u1)\n",
    "y_inbuilt = inbuiltLayer(u2)\n",
    "\n",
    "print(y_custom)\n",
    "\n",
    "loss_custom = lossFunc(y_custom, torch.zeros_like(y_custom))\n",
    "loss_in = lossFunc(y_inbuilt, torch.zeros_like(y_inbuilt))\n",
    "\n",
    "loss_custom.backward()\n",
    "loss_in.backward()\n",
    "\n",
    "print(\"gradients of loss relative to the input\")\n",
    "print(u1.grad)\n",
    "print(u2.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb0ae69",
   "metadata": {},
   "source": [
    "### Part 2: MLP Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d10d7c7",
   "metadata": {},
   "source": [
    "Now that you have implemented an MLP from scratch, it's time to train it and verify its ability to classify objects. This network is expected to achieve an accuracy of approximately 40%.\n",
    "\n",
    "Additionally, you are required to save one of your best model checkpoints as mlp.pth in the current folder. This file will be used for automated testing.\n",
    "\n",
    "Furthermore, please implement a confusion matrix in the utils file, specifically within the val_step method of the Pipeline class. You may use any available implementation of the confusion matrix, but ensure that all tests continue to pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets train a CIFAR10 image classifier\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import networks as net\n",
    "import os\n",
    "importlib.reload(net)\n",
    "\n",
    "pipeline = net.Pipeline()\n",
    "model = net.CustomMLP().to(pipeline.device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "home_path = os.path.expanduser(\"~\")\n",
    "JOB_FOLDER=os.path.join(home_path, \"outputs/\")\n",
    "TRAINED_MDL_PATH = os.path.join(JOB_FOLDER, \"cifar/mlp/\")\n",
    "\n",
    "import os\n",
    "os.makedirs(JOB_FOLDER, exist_ok=True)\n",
    "os.makedirs(TRAINED_MDL_PATH, exist_ok=True)\n",
    "\n",
    "epochs = 40\n",
    "trainLossList = []\n",
    "valAccList = []\n",
    "for eIndex in range(epochs):\n",
    "    # print(\"Epoch count: \", eIndex)\n",
    "    \n",
    "    train_epochloss = pipeline.train_step(model, optimizer)\n",
    "    val_acc = pipeline.val_step(model)\n",
    "\n",
    "    print(eIndex, train_epochloss, val_acc)\n",
    "\n",
    "    valAccList.append(val_acc)\n",
    "    trainLossList.append(train_epochloss)\n",
    "\n",
    "    trainedMdlPath = TRAINED_MDL_PATH + f\"{eIndex}.pth\"\n",
    "    torch.save(model.state_dict(), trainedMdlPath)\n",
    "\n",
    "trainLosses = np.array(trainLossList)\n",
    "testAccuracies = np.array(valAccList)\n",
    "\n",
    "np.savetxt(\"train.log\", trainLosses)\n",
    "np.savetxt(\"test.log\", testAccuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b1e2d",
   "metadata": {},
   "source": [
    "### Part 3: Implement Convolutional Neural Networks (CNN) Using PyTorch layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37c6b38",
   "metadata": {},
   "source": [
    "CNNs excel in capturing local patterns and spatial hierarchies through convolutional filters, which makes them more effective for image and spatial data. They also use parameter sharing, reducing the number of parameters and computational cost compared to MLPs. Additionally, CNNs offer translation invariance and hierarchical feature learning, enabling them to recognize features across different spatial locations and build complex patterns efficiently.\n",
    "\n",
    "Open networks.py and implement `RefCNN` using the inbuilt layers in pytorch. Make sure it is similar to CustomCNN() which uses custom layers.\n",
    "\n",
    "Train and compare the train loss and validation accuracy against MLP. \n",
    "\n",
    "Please copy the best checkpoint file in current folder as cnn_inbuilt.pth for automated tests. It is expected to be higher than 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a9cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets train a CIFAR10 image classifier\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import networks as net\n",
    "import os\n",
    "importlib.reload(net)\n",
    "\n",
    "pipeline = net.Pipeline()\n",
    "model = net.RefCNN().to(pipeline.device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "home_path = os.path.expanduser(\"~\")\n",
    "JOB_FOLDER=os.path.join(home_path, \"outputs/\")\n",
    "TRAINED_MDL_PATH = os.path.join(JOB_FOLDER, \"cifar/cnn_inbuilt_layers/\")\n",
    "\n",
    "import os\n",
    "os.makedirs(JOB_FOLDER, exist_ok=True)\n",
    "os.makedirs(TRAINED_MDL_PATH, exist_ok=True)\n",
    "\n",
    "epochs = 40\n",
    "trainLossList = []\n",
    "valAccList = []\n",
    "for eIndex in range(epochs):\n",
    "    # print(\"Epoch count: \", eIndex)\n",
    "    \n",
    "    train_epochloss = pipeline.train_step(model, optimizer)\n",
    "    print(\"train complete\")\n",
    "    val_acc = pipeline.val_step(model)\n",
    "\n",
    "    print(eIndex, train_epochloss, val_acc)\n",
    "\n",
    "    valAccList.append(val_acc)\n",
    "    trainLossList.append(train_epochloss)\n",
    "\n",
    "    trainedMdlPath = TRAINED_MDL_PATH + f\"{eIndex}.pth\"\n",
    "    torch.save(model.state_dict(), trainedMdlPath)\n",
    "\n",
    "trainLosses = np.array(trainLossList)\n",
    "testAccuracies = np.array(valAccList)\n",
    "\n",
    "np.savetxt(\"train.log\", trainLosses)\n",
    "np.savetxt(\"test.log\", testAccuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9b9224",
   "metadata": {},
   "source": [
    "### Part 4: Implement Your Custom Layers for Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e78b4fe",
   "metadata": {},
   "source": [
    "Open custom_layers.py and implement the CustomConvLayer.\n",
    "\n",
    "Verify it by running the below code. Feel free to modify the below snippet. But do not modify my test.py\n",
    "\n",
    "For more information about supplying gradients, please refer to [examples_autograd](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23dc62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import networks as net\n",
    "import os\n",
    "importlib.reload(net)\n",
    "\n",
    "inbuiltLayer = nn.Conv2d(2, 3, 3, stride=2, padding='valid')\n",
    "customLayer = net.CustomConv2d(2, 3, 3, 2)\n",
    "\n",
    "inbuiltLayer.weight.data.copy_(customLayer.weight.data)\n",
    "inbuiltLayer.bias.data.copy_(customLayer.bias.data)\n",
    "\n",
    "u1 = torch.rand((1, 2, 5, 5), requires_grad=True)\n",
    "u2 = u1.detach().clone()\n",
    "u2.requires_grad_()\n",
    "\n",
    "y1 = inbuiltLayer(u1)\n",
    "y2 = customLayer(u2)\n",
    "\n",
    "print(\"Conv. Inference\")\n",
    "print(y1)\n",
    "print(y2)\n",
    "\n",
    "lossFunc = nn.MSELoss()\n",
    "loss_custom = lossFunc(y2, torch.zeros_like(y2))\n",
    "loss_in = lossFunc(y1, torch.zeros_like(y1))\n",
    "\n",
    "loss_in.backward()\n",
    "loss_custom.backward()\n",
    "\n",
    "print(\"gradients of loss relative to the weights\")\n",
    "print(inbuiltLayer.weight.grad)\n",
    "print(customLayer.weight.grad)\n",
    "\n",
    "print(\"gradients of loss relative to the bias\")\n",
    "print(inbuiltLayer.bias.grad)\n",
    "print(customLayer.bias.grad)\n",
    "\n",
    "print(\"gradients of loss relative to the input\")\n",
    "print(u1.grad)\n",
    "print(u2.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf028b78",
   "metadata": {},
   "source": [
    "### Part 5: CNN Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ca731",
   "metadata": {},
   "source": [
    "Train and compare the train loss and validation accuracy against MLP and inbuilt conv layers. \n",
    "\n",
    "Please copy the best checkpoint file in current folder as `cnn_custom.pth` for automated tests. It is expected to be higher than 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f31b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets train a CIFAR10 image classifier\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import networks as net\n",
    "import os\n",
    "importlib.reload(net)\n",
    "\n",
    "pipeline = net.Pipeline()\n",
    "model = net.CustomCNN().to(pipeline.device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "home_path = os.path.expanduser(\"~\")\n",
    "JOB_FOLDER=os.path.join(home_path, \"outputs/\")\n",
    "TRAINED_MDL_PATH = os.path.join(JOB_FOLDER, \"cifar/cnn_custom_layer/\")\n",
    "\n",
    "import os\n",
    "os.makedirs(JOB_FOLDER, exist_ok=True)\n",
    "os.makedirs(TRAINED_MDL_PATH, exist_ok=True)\n",
    "\n",
    "epochs = 40\n",
    "trainLossList = []\n",
    "valAccList = []\n",
    "for eIndex in range(epochs):\n",
    "    # print(\"Epoch count: \", eIndex)\n",
    "    \n",
    "    train_epochloss = pipeline.train_step(model, optimizer)\n",
    "    print(\"train complete\")\n",
    "    val_acc = pipeline.val_step(model)\n",
    "\n",
    "    print(eIndex, train_epochloss, val_acc)\n",
    "\n",
    "    valAccList.append(val_acc)\n",
    "    trainLossList.append(train_epochloss)\n",
    "\n",
    "    trainedMdlPath = TRAINED_MDL_PATH + f\"{eIndex}.pth\"\n",
    "    torch.save(model.state_dict(), trainedMdlPath)\n",
    "\n",
    "trainLosses = np.array(trainLossList)\n",
    "testAccuracies = np.array(valAccList)\n",
    "\n",
    "np.savetxt(\"train.log\", trainLosses)\n",
    "np.savetxt(\"test.log\", testAccuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54272187",
   "metadata": {},
   "source": [
    "## 5. Grading Rubric\n",
    "\n",
    "- part 1 : 60\n",
    "- part 2 : 10\n",
    "- part 3 : 10\n",
    "- part 4 : 10\n",
    "- part 5 : 10\n",
    "\n",
    "For RBE474X: part1 + part2 + part3 = 100% of the grade (80/80).\n",
    "For RBE595-A01-SP: You are expected to implement part1-part5 for getting full credits (100/100).\n",
    "\n",
    "Your code will be evaluated with test.py. Please run it and ensure that the tests pass before submitting. Instructions are in software setup section.\n",
    "\n",
    "Please note that I will replace the test.py with my original test.py before evaluating.\n",
    "\n",
    "Please do not submit the data folder that is downloaded while training the network. It is over 300 MB. Anyone submitting data will be penalized! Your submission should not be more than 20 MB.\n",
    "\n",
    "## 6. Report Guidelines\n",
    "\n",
    "Report must be in Latex.\n",
    "\n",
    "Include the following,\n",
    "\n",
    "1. Training loss curve (loss vs epoch count)\n",
    "2. Confusion Matrix for validation set (val_step)\n",
    "3. Accuracy comparison between MLP, CNN (torch layers) and CNN (custom_layers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
